{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26dc1fa6",
   "metadata": {},
   "source": [
    "# Yelp Sentiment Analysis - Data Preprocessing Utilities\n",
    "\n",
    "This notebook contains utility functions and classes for preprocessing Yelp restaurant and hotel reviews for sentiment analysis using DistilBERT.\n",
    "\n",
    "## Project Overview\n",
    "- **Dataset**: Yelp restaurant and hotel reviews\n",
    "- **Task**: Sentiment analysis (positive, negative, neutral)\n",
    "- **Model**: DistilBERT for text classification\n",
    "- **Preprocessing**: Text cleaning, label encoding, data splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8491321c",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549106ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from typing import Tuple, List\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a627138a",
   "metadata": {},
   "source": [
    "## 2. YelpDataProcessor Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4d8174",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YelpDataProcessor:\n",
    "    \"\"\"Data processor for Yelp reviews dataset.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.label_map = None\n",
    "        \n",
    "    def load_data(self, file_path: str) -> pd.DataFrame:\n",
    "        \"\"\"Load Yelp dataset from CSV file.\n",
    "        \n",
    "        Args:\n",
    "            file_path: Path to the CSV file\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with loaded data\n",
    "        \"\"\"\n",
    "        logger.info(f\"Loading data from {file_path}\")\n",
    "        df = pd.read_csv(file_path)\n",
    "        logger.info(f\"Loaded dataset with shape: {df.shape}\")\n",
    "        return df\n",
    "    \n",
    "    def clean_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Clean the dataset by removing missing values and filtering labels.\n",
    "        \n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "            \n",
    "        Returns:\n",
    "            Cleaned DataFrame\n",
    "        \"\"\"\n",
    "        logger.info(\"Cleaning data...\")\n",
    "        \n",
    "        # Drop rows with missing cleaned_text or sentiment label\n",
    "        df_clean = df.dropna(subset=[\"cleaned_text\", \"star_sentiment\"])\n",
    "        \n",
    "        # Keep only reviews labeled as positive, negative, or neutral\n",
    "        df_clean = df_clean[df_clean[\"star_sentiment\"].isin([\"positive\", \"negative\", \"neutral\"])]\n",
    "        \n",
    "        # Reset index\n",
    "        df_clean.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        logger.info(f\"Cleaned dataset shape: {df_clean.shape}\")\n",
    "        logger.info(f\"Label distribution:\\n{df_clean['star_sentiment'].value_counts()}\")\n",
    "        \n",
    "        return df_clean\n",
    "    \n",
    "    def encode_labels(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Encode sentiment labels to numerical values.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame with sentiment labels\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with encoded labels\n",
    "        \"\"\"\n",
    "        logger.info(\"Encoding labels...\")\n",
    "        \n",
    "        df = df.copy()\n",
    "        df[\"label\"] = self.label_encoder.fit_transform(df[\"star_sentiment\"])\n",
    "        \n",
    "        # Create label mapping\n",
    "        self.label_map = dict(zip(\n",
    "            self.label_encoder.classes_, \n",
    "            self.label_encoder.transform(self.label_encoder.classes_)\n",
    "        ))\n",
    "        \n",
    "        logger.info(f\"Label encoding: {self.label_map}\")\n",
    "        return df\n",
    "    \n",
    "    def split_data(self, df: pd.DataFrame, test_size: float = 0.2, val_size: float = 0.1, \n",
    "                   random_state: int = 42) -> Tuple[pd.Series, pd.Series, pd.Series, \n",
    "                                                   np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"Split data into train, validation, and test sets.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame with text and labels\n",
    "            test_size: Proportion of test set\n",
    "            val_size: Proportion of validation set\n",
    "            random_state: Random seed\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (train_texts, val_texts, test_texts, train_labels, val_labels, test_labels)\n",
    "        \"\"\"\n",
    "        logger.info(\"Splitting data into train/val/test sets...\")\n",
    "        \n",
    "        # Split into train (80%) and temp (20%)\n",
    "        train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
    "            df[\"cleaned_text\"], df[\"label\"], \n",
    "            test_size=test_size, stratify=df[\"label\"], random_state=random_state\n",
    "        )\n",
    "        \n",
    "        # Split temp into val (10%) and test (10%)\n",
    "        val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
    "            temp_texts, temp_labels, \n",
    "            test_size=0.5, stratify=temp_labels, random_state=random_state\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Train size: {len(train_texts)}\")\n",
    "        logger.info(f\"Validation size: {len(val_texts)}\")\n",
    "        logger.info(f\"Test size: {len(test_texts)}\")\n",
    "        \n",
    "        return train_texts, val_texts, test_texts, train_labels.values, val_labels.values, test_labels.values\n",
    "    \n",
    "    def get_label_names(self) -> List[str]:\n",
    "        \"\"\"Get list of label names.\n",
    "        \n",
    "        Returns:\n",
    "            List of label names\n",
    "        \"\"\"\n",
    "        if self.label_encoder.classes_ is not None:\n",
    "            return self.label_encoder.classes_.tolist()\n",
    "        return [\"negative\", \"neutral\", \"positive\"]\n",
    "    \n",
    "    def visualize_label_distribution(self, df: pd.DataFrame, title: str = \"Label Distribution\"):\n",
    "        \"\"\"Visualize the distribution of sentiment labels.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame with sentiment labels\n",
    "            title: Title for the plot\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # Count plot\n",
    "        plt.subplot(1, 2, 1)\n",
    "        df['star_sentiment'].value_counts().plot(kind='bar', color=['#ff7f7f', '#7fbf7f', '#7f7fff'])\n",
    "        plt.title(f'{title} - Counts')\n",
    "        plt.xlabel('Sentiment')\n",
    "        plt.ylabel('Count')\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        # Pie chart\n",
    "        plt.subplot(1, 2, 2)\n",
    "        df['star_sentiment'].value_counts().plot(kind='pie', autopct='%1.1f%%', \n",
    "                                                colors=['#ff7f7f', '#7fbf7f', '#7f7fff'])\n",
    "        plt.title(f'{title} - Proportions')\n",
    "        plt.ylabel('')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def save_processed_data(self, train_texts, val_texts, test_texts, \n",
    "                           train_labels, val_labels, test_labels, output_dir: str):\n",
    "        \"\"\"Save processed data to pickle files.\n",
    "        \n",
    "        Args:\n",
    "            train_texts, val_texts, test_texts: Text data splits\n",
    "            train_labels, val_labels, test_labels: Label data splits\n",
    "            output_dir: Directory to save the files\n",
    "        \"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Save text data\n",
    "        with open(os.path.join(output_dir, 'train_texts.pkl'), 'wb') as f:\n",
    "            pickle.dump(train_texts, f)\n",
    "        with open(os.path.join(output_dir, 'val_texts.pkl'), 'wb') as f:\n",
    "            pickle.dump(val_texts, f)\n",
    "        with open(os.path.join(output_dir, 'test_texts.pkl'), 'wb') as f:\n",
    "            pickle.dump(test_texts, f)\n",
    "            \n",
    "        # Save labels\n",
    "        with open(os.path.join(output_dir, 'train_labels.pkl'), 'wb') as f:\n",
    "            pickle.dump(train_labels, f)\n",
    "        with open(os.path.join(output_dir, 'val_labels.pkl'), 'wb') as f:\n",
    "            pickle.dump(val_labels, f)\n",
    "        with open(os.path.join(output_dir, 'test_labels.pkl'), 'wb') as f:\n",
    "            pickle.dump(test_labels, f)\n",
    "            \n",
    "        # Save label encoder\n",
    "        with open(os.path.join(output_dir, 'label_encoder.pkl'), 'wb') as f:\n",
    "            pickle.dump(self.label_encoder, f)\n",
    "            \n",
    "        logger.info(f\"Processed data saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dab796b",
   "metadata": {},
   "source": [
    "## 3. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ea6f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_yelp_data(file_path: str) -> Tuple[pd.Series, pd.Series, pd.Series, \n",
    "                                               np.ndarray, np.ndarray, np.ndarray, \n",
    "                                               YelpDataProcessor]:\n",
    "    \"\"\"Complete data processing pipeline.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the Yelp dataset CSV\n",
    "        \n",
    "    Returns:\n",
    "        Processed train/val/test data and the processor instance\n",
    "    \"\"\"\n",
    "    processor = YelpDataProcessor()\n",
    "    \n",
    "    # Load and process data\n",
    "    df = processor.load_data(file_path)\n",
    "    df_clean = processor.clean_data(df)\n",
    "    df_encoded = processor.encode_labels(df_clean)\n",
    "    \n",
    "    # Split data\n",
    "    train_texts, val_texts, test_texts, train_labels, val_labels, test_labels = processor.split_data(df_encoded)\n",
    "    \n",
    "    return train_texts, val_texts, test_texts, train_labels, val_labels, test_labels, processor\n",
    "\n",
    "def load_processed_data(data_dir: str):\n",
    "    \"\"\"Load previously processed data from pickle files.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Directory containing the processed data files\n",
    "        \n",
    "    Returns:\n",
    "        Loaded data splits and label encoder\n",
    "    \"\"\"\n",
    "    # Load text data\n",
    "    with open(os.path.join(data_dir, 'train_texts.pkl'), 'rb') as f:\n",
    "        train_texts = pickle.load(f)\n",
    "    with open(os.path.join(data_dir, 'val_texts.pkl'), 'rb') as f:\n",
    "        val_texts = pickle.load(f)\n",
    "    with open(os.path.join(data_dir, 'test_texts.pkl'), 'rb') as f:\n",
    "        test_texts = pickle.load(f)\n",
    "        \n",
    "    # Load labels\n",
    "    with open(os.path.join(data_dir, 'train_labels.pkl'), 'rb') as f:\n",
    "        train_labels = pickle.load(f)\n",
    "    with open(os.path.join(data_dir, 'val_labels.pkl'), 'rb') as f:\n",
    "        val_labels = pickle.load(f)\n",
    "    with open(os.path.join(data_dir, 'test_labels.pkl'), 'rb') as f:\n",
    "        test_labels = pickle.load(f)\n",
    "        \n",
    "    # Load label encoder\n",
    "    with open(os.path.join(data_dir, 'label_encoder.pkl'), 'rb') as f:\n",
    "        label_encoder = pickle.load(f)\n",
    "        \n",
    "    return train_texts, val_texts, test_texts, train_labels, val_labels, test_labels, label_encoder\n",
    "\n",
    "def display_data_info(train_texts, val_texts, test_texts, train_labels, val_labels, test_labels):\n",
    "    \"\"\"Display information about the data splits.\n",
    "    \n",
    "    Args:\n",
    "        Data splits to analyze\n",
    "    \"\"\"\n",
    "    print(\"=== Data Split Information ===\")\n",
    "    print(f\"Training set size: {len(train_texts)}\")\n",
    "    print(f\"Validation set size: {len(val_texts)}\")\n",
    "    print(f\"Test set size: {len(test_texts)}\")\n",
    "    print(f\"Total samples: {len(train_texts) + len(val_texts) + len(test_texts)}\")\n",
    "    \n",
    "    print(\"\\n=== Label Distribution ===\")\n",
    "    print(\"Training set:\")\n",
    "    unique, counts = np.unique(train_labels, return_counts=True)\n",
    "    for label, count in zip(unique, counts):\n",
    "        print(f\"  Label {label}: {count} ({count/len(train_labels)*100:.1f}%)\")\n",
    "    \n",
    "    print(\"Validation set:\")\n",
    "    unique, counts = np.unique(val_labels, return_counts=True)\n",
    "    for label, count in zip(unique, counts):\n",
    "        print(f\"  Label {label}: {count} ({count/len(val_labels)*100:.1f}%)\")\n",
    "    \n",
    "    print(\"Test set:\")\n",
    "    unique, counts = np.unique(test_labels, return_counts=True)\n",
    "    for label, count in zip(unique, counts):\n",
    "        print(f\"  Label {label}: {count} ({count/len(test_labels)*100:.1f}%)\")\n",
    "\n",
    "# Example usage function\n",
    "def run_preprocessing_example():\n",
    "    \"\"\"Example function showing how to use the preprocessing utilities.\"\"\"\n",
    "    print(\"=== Yelp Data Preprocessing Example ===\")\n",
    "    print(\"1. Load the data using: processor.load_data('path/to/data.csv')\")\n",
    "    print(\"2. Clean the data using: processor.clean_data(df)\")\n",
    "    print(\"3. Encode labels using: processor.encode_labels(df)\")\n",
    "    print(\"4. Split data using: processor.split_data(df)\")\n",
    "    print(\"5. Visualize using: processor.visualize_label_distribution(df)\")\n",
    "    print(\"6. Save processed data using: processor.save_processed_data(...)\")\n",
    "    print(\"\\nOr use the complete pipeline: process_yelp_data('path/to/data.csv')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f509e2",
   "metadata": {},
   "source": [
    "## 4. Example Usage\n",
    "\n",
    "This section shows how to use the preprocessing utilities. Uncomment and run the cells below to test the functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c26e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show example usage\n",
    "run_preprocessing_example()\n",
    "\n",
    "# Example: Process the Yelp data (uncomment to run)\n",
    "# data_path = \"../data/yelp_restaurants_hotels_ver2.csv\"\n",
    "# train_texts, val_texts, test_texts, train_labels, val_labels, test_labels, processor = process_yelp_data(data_path)\n",
    "\n",
    "# Display information about the processed data\n",
    "# display_data_info(train_texts, val_texts, test_texts, train_labels, val_labels, test_labels)\n",
    "\n",
    "# Save processed data\n",
    "# processor.save_processed_data(train_texts, val_texts, test_texts, \n",
    "#                              train_labels, val_labels, test_labels, \"../outputs/processed_data\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
